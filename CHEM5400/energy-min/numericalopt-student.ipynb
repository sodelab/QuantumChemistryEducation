{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Numerical Optimization\"\"\"\n",
    "\n",
    "__authors__ = [\"Olaseni Sode\"]\n",
    "__email__   = [\"osode@calstatela.edu\"]\n",
    "__date__      = \"2022-08-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of Assignment 1, you will determine the minimum of a multivariable function using mathematical optimization techniques. This task is designed to help you understand how optimization methods are applied to more complex functions, which is fundamental in computational chemistry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Your Mathematical Function\n",
    " \n",
    "Your first task is to define the function you will be optimizing. Replace the function below with your assigned mathematical function from the Canvas page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports **NumPy** for numerical operations and **SciPy**â€™s linear algebra module for advanced mathematical functions needed for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python modules that we will use\n",
    "import numpy as np\n",
    "from scipy import linalg as splinalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x,y):\n",
    "    #z = ___________ # Fill in: your mathematical function\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot your function:**\n",
    "Visualize your function to understand its shape and identify any potential minima or maxima. This is straightforward with mathematical functions since we have an explicit expression. However, in real molecular calculations, we can't visualize the potential energy surface (PES) beforehand; optimization is needed to explore the PES directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "x = np.linspace(-6, 6, 150)\n",
    "y = np.linspace(-6, 6, 150)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = function(X, Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 60, cmap='binary')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "# Don't forget to title your graph\n",
    "ax.set_title('________') # Fill in: the title of your graph\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "\n",
    "To find the minimum of your function, you'll start with the **Gradient Descent** algorithm, a simple yet powerful optimization method that uses the first derivatives (gradients) of the function to iteratively move towards the minimum.\n",
    "\n",
    "#### **How Gradient Descent Works:**\n",
    "\n",
    "1. **Initialize Starting Point:**\n",
    "   Choose an initial point $ (x_0, y_0) $ as your starting coordinates.\n",
    "\n",
    "2. **Compute the Gradient:**\n",
    "   At each iteration, compute the gradient of the function:\n",
    "   $\\nabla f(x, y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} $\n",
    "   The gradient points in the direction of the steepest increase in the function value.\n",
    "\n",
    "3. **Update the Coordinates:**\n",
    "   Update the coordinates by moving in the opposite direction of the gradient:\n",
    "   $$\n",
    "   \\begin{bmatrix} x_{n+1} \\\\ y_{n+1} \\end{bmatrix} = \\begin{bmatrix} x_{n} \\\\ y_{n} \\end{bmatrix} - \\alpha \\cdot \\nabla f(x_n, y_n)\n",
    "   $$\n",
    "   Here, $ \\alpha $ is the **learning rate**, a small positive constant that determines the step size for each update.\n",
    "\n",
    "4. **Iterate Until Convergence:**\n",
    "   Repeat steps 2-3 until the gradient becomes very small or reaches a predefined threshold, indicating that you are close to a minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Gradient of the Function\n",
    "\n",
    "To implement the gradient descent algorithm, we need to compute the gradient (partial derivatives) of the function with respect to $x$ and $y$. The gradient indicates the direction and rate of the steepest increase in the function's value. By moving in the opposite direction of the gradient, we can iteratively approach the minimum of the function.\n",
    "\n",
    "In this cell, you will define a Python function to calculate the partial derivatives, $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$, which are essential for updating the coordinates during the optimization process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate partial derivatives (gradients) of the function\n",
    "def gradient(x, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the function at point (x, y).\n",
    "    \n",
    "    Returns:\n",
    "    A 1x2 numpy array representing the gradient.\n",
    "    \"\"\"\n",
    "    # Fill in: calculate the partial derivative with respect to x\n",
    "    df_dx = ___________\n",
    "    \n",
    "    # Fill in: calculate the partial derivative with respect to y\n",
    "    df_dy = ___________\n",
    "\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Gradient Descent Function\n",
    "\n",
    "Now that you understand the steps involved in the Gradient Descent algorithm, you will create a Python function to perform the optimization. Your function should accept several input parameters: the initial coordinates $(x_0, y_0)$, the learning rate $\\alpha$, the maximum number of iterations, and the convergence tolerance. Using these inputs, the function will iteratively update the coordinates by calculating the gradient at the current coordinates with the gradient function you defined earlier and applying the update formula mentioned previously. \n",
    "\n",
    "The function should continue iterating until the changes in coordinates are smaller than the specified tolerance level or until the maximum number of iterations is reached, indicating convergence. Finally, the function should return the final coordinates, the function value at the minimum, and the number of iterations taken. Implement this function in the next code cell and run it to find the minimum of your assigned function. Adjust the parameters, such as the learning rate and initial coordinates, and observe how they impact the optimization process.\n",
    "\n",
    "In the function definition, `learning_rate=0.01` specifies a **default value** for the `learning_rate` parameter. This means that if you call the function without providing a specific value for `learning_rate`, the function will automatically use `0.01` as the value.\n",
    "\n",
    "For example, if you call the function like this:\n",
    "\n",
    "```python\n",
    "gradient_descent(func, grad_func, initial_x, initial_y)\n",
    "```\n",
    "\n",
    "The function will use learning_rate = 0.01 by default. However, if you want to use a different learning rate, you can specify it when calling the function:\n",
    "```python\n",
    "gradient_descent(func, grad_func, initial_x, initial_y, learning_rate=0.05)\n",
    "```\n",
    "For more information on how default values work in Python functions, you can refer to the [official Python documentation on defining functions](https://docs.python.org/3/tutorial/controlflow.html#more-on-defining-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(func, grad_func, initial_x, initial_y, learning_rate=0.01, max_iterations=10000, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Perform Gradient Descent to find the minimum of a function.\n",
    "    \n",
    "    Parameters:\n",
    "    - func: The function to minimize.\n",
    "    - grad_func: The function to compute the gradient.\n",
    "    - initial_x: Initial x-coordinate.\n",
    "    - initial_y: Initial y-coordinate.\n",
    "    - learning_rate: Step size for each iteration (default is 0.01).\n",
    "    - max_iterations: Maximum number of iterations to attempt (default is 10000).\n",
    "    - tolerance: Convergence threshold (default is 1e-4).\n",
    "    \n",
    "    Returns:\n",
    "    - (x, y): Coordinates of the function minimum.\n",
    "    - f_min: Minimum value of the function.\n",
    "    - iterations: Number of iterations performed.\n",
    "    \"\"\"\n",
    "    # Initialize starting point\n",
    "    x, y = initial_x, initial_y\n",
    "    \n",
    "    # Iterate until convergence or until reaching the maximum number of iterations\n",
    "    for i in range(___________):  # Fill in: specify the range for maximum iterations\n",
    "        # Calculate the gradient at the current point\n",
    "        grad = grad_func(_________)  # Fill in: pass the correct arguments to grad_func\n",
    "        \n",
    "        # Update the coordinates by moving in the opposite direction of the gradient\n",
    "        x_new = ___________  # Fill in: update x using the learning rate and gradient\n",
    "        y_new = ___________  # Fill in: update y using the learning rate and gradient\n",
    "        \n",
    "        # Check for convergence\n",
    "        if ___________:  # Fill in: define the condition to check if the change is below the tolerance\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            return (x_new, y_new), func(x_new, y_new), i+1\n",
    "        \n",
    "        # Update the coordinates\n",
    "        x, y = ___________  # Fill in: update x and y with the new values\n",
    "    \n",
    "    print(\"Reached maximum iterations without full convergence.\")\n",
    "    return (x, y), func(x, y), max_iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will set the initial parameters for the Gradient Descent algorithm. Choose random initial coordinates for `initial_x` and `initial_y`. Also specify the `learning_rate`. It should probably be a number less than 0.05. The `gradient_descent` function is then run with these parameters to find the minimum of the function, and the results, including the minimum coordinates, function value at the minimum, and the number of iterations taken, are printed for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial parameters\n",
    "initial_x, initial_y = ___________,___________  # Fill in: initial values for x and y\n",
    "\n",
    "print(f\"Initial coordinates at: x = {initial_x:.4f}, y = {initial_y:.4f}\\n\")\n",
    "\n",
    "learning_rate = ___________  # Fill in: initial learning rate lower than 0.1\n",
    "\n",
    "# Run the gradient descent function\n",
    "min_coords, min_value, iterations = gradient_descent(function, gradient, initial_x, initial_y, learning_rate)\n",
    "\n",
    "print(f\"Minimum found at: x = {min_coords[0]:.4f}, y = {min_coords[1]:.4f}\")\n",
    "print(f\"Function value at minimum: {min_value:.4f}\")\n",
    "print(f\"Number of iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** **<span style=\"color:red\">What happens to the convergence of the algorithm if the learning rate $\\alpha$ is too large or too small? Explain why these changes occur and how they affect the speed and accuracy of finding the minimum.</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "**<span style=\"color:red\">How does the choice of initial coordinates $(x_0, y_0)$ influence the algorithm's ability to find the global minimum versus getting stuck in a local minimum? Can you think of strategies to mitigate this risk?</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** **<span style=\"color:red\">Why is it important to define a convergence threshold (tolerance)? How does changing the tolerance affect the number of iterations and the final result? What might happen if the tolerance is set too high or too low?</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Newton-Raphson\n",
    "\n",
    "Next, you will implement the **Newton-Raphson** method, a more advanced optimization algorithm that uses both the first and second derivatives of the function to find its minimum.\n",
    "\n",
    "#### **How the Newton-Raphson Method Works:**\n",
    "\n",
    "a. **Initialize Starting Point:**\n",
    "   Start with an initial point $(x_0, y_0)$, chosen randomly or manually.\n",
    "\n",
    "b. **Compute the Gradient and Hessian:** At each iteration, compute the gradient vector:\n",
    "\n",
    "   $$\n",
    "   \\nabla f(x, y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   and the Hessian matrix, which contains the second derivatives:\n",
    "\n",
    "   $$\n",
    "   H(x, y) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "c. **Update the Coordinates:** Use the Newton-Raphson update rule to adjust the coordinates:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix} x_{n+1} \\\\ y_{n+1} \\end{bmatrix} = \\begin{bmatrix} x_{n} \\\\ y_{n} \\end{bmatrix} - H(x, y)^{-1} \\nabla f(x, y)\n",
    "   $$\n",
    "\n",
    "d. **Iterate Until Convergence:** Repeat the process until the updates are smaller than a predefined tolerance or the maximum number of iterations is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compute the Hessian of the Function**\n",
    "\n",
    "Since you've already implemented the gradient of your function, we need to compute the **Hessian matrix** of the function, which consists of all the second-order partial derivatives with respect to $x$ and $y$. The Hessian provides information about the curvature of the function and is used to make more accurate updates to the coordinates when searching for the minimum.\n",
    "\n",
    "In this cell, you will define a Python function to calculate the Hessian matrix, including the following second derivatives: $\\frac{\\partial^2 f}{\\partial x^2}$, $\\frac{\\partial^2 f}{\\partial y^2}$, and the mixed partial derivatives $\\frac{\\partial^2 f}{\\partial x \\partial y}$ and $\\frac{\\partial^2 f}{\\partial y \\partial x}$. These values are essential for determining the optimal update direction and step size in the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to compute the Hessian matrix\n",
    "def hessian(x, y):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix for the function at point (x, y).\n",
    "    \n",
    "    Returns:\n",
    "    A 2x2 numpy array representing the Hessian matrix.\n",
    "    \"\"\"\n",
    "    # Compute the second partial derivatives\n",
    "    # Fill in: calculate the second partial derivative with respect to x\n",
    "    d2f_dx2 = ___________\n",
    "\n",
    "    # Fill in: calculate the mixed partial derivative with respect to x and y\n",
    "    d2f_dxdy = ___________\n",
    "\n",
    "    # Fill in: calculate the mixed partial derivative with respect to y and x (equal to d2f_dxdy)\n",
    "    d2f_dydx = ___________\n",
    "\n",
    "    # Fill in: calculate the second partial derivative with respect to y\n",
    "    d2f_dy2 = ___________\n",
    "\n",
    "    # Create the Hessian matrix\n",
    "    H = np.array([[d2f_dx2, d2f_dxdy],\n",
    "                  [d2f_dydx, d2f_dy2]])\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define the Newton-Raphson Function:**\n",
    "\n",
    "Now that you understand the steps involved in the Newton-Raphson method, you will create a Python function to perform the optimization. Your function should accept input parameters such as the initial coordinates $(x_0, y_0)$, the maximum number of iterations, and the convergence tolerance. The function will use the gradient function to calculate the gradient vector and the Hessian function to compute the Hessian matrix at the current coordinates. It will iteratively update the coordinates using the Newton-Raphson formula:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_{n+1} \\\\ y_{n+1} \\end{bmatrix} = \\begin{bmatrix} x_{n} \\\\ y_{n} \\end{bmatrix} - H(x, y)^{-1} \\nabla f(x, y)\n",
    "$$\n",
    "\n",
    "The function should continue updating the coordinates until the change is smaller than the specified tolerance level or the maximum number of iterations is reached. Finally, the function should return the final coordinates, the function value at the minimum, and the number of iterations taken. Implement this function in the next code cell and run it to find the minimum of your assigned function. Experiment with different initial coordinates and convergence criteria to see how they impact the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson(func, grad_func, hessian_func, initial_x, initial_y, max_iterations=1000, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Perform the Newton-Raphson method to find the minimum of a function.\n",
    "    \n",
    "    Parameters:\n",
    "    - func: The function to minimize.\n",
    "    - grad_func: The function to compute the gradient.\n",
    "    - hessian_func: The function to compute the Hessian matrix.\n",
    "    - initial_x: Initial x-coordinate.\n",
    "    - initial_y: Initial y-coordinate.\n",
    "    - max_iterations: Maximum number of iterations to attempt (default is 1000).\n",
    "    - tolerance: Convergence threshold (default is 1e-4).\n",
    "    \n",
    "    Returns:\n",
    "    - (x, y): Coordinates of the function minimum.\n",
    "    - f_min: Minimum value of the function.\n",
    "    - iterations: Number of iterations performed.\n",
    "    \"\"\"\n",
    "    # Initialize starting point\n",
    "    x, y = initial_x, initial_y\n",
    "\n",
    "    # Iterate until convergence or until reaching the maximum number of iterations\n",
    "    for i in range(___________):\n",
    "        # Compute the gradient and Hessian matrix at the current point\n",
    "        grad = grad_func(___________) # Fill in: pass the correct arguments to grad_func\n",
    "        hess = hessian_func(___________) # Fill in: pass the correct arguments to hessian_func\n",
    "\n",
    "        # Update the coordinates\n",
    "        x_new = ___________  # Fill in: update x using the learning rate and gradient\n",
    "        y_new = ___________  # Fill in: update y using the learning rate and gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if ___________:  # Fill in: define the condition to check if the change is below the tolerance\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            return (x_new, y_new), func(x_new, y_new), i+1\n",
    "\n",
    "        # Update the coordinates\n",
    "        x, y = ___________  # Fill in: update x and y with the new values\n",
    "\n",
    "    print(\"Reached maximum iterations without full convergence.\")\n",
    "    return (x, y), func(x, y), max_iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will set the initial parameters for the Newton-Raphson algorithm. Choose random initial coordinates for `initial_x` and `initial_y`. The `newton_raphson` function is then run with these parameters to find the minimum of the function, and the results, including the minimum coordinates, function value at the minimum, and the number of iterations taken, are printed for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial parameters\n",
    "initial_x, initial_y = ___________,___________  # Fill in: initial values for x and y\n",
    "\n",
    "print(f\"Initial coordinates at: x = {initial_x:.4f}, y = {initial_y:.4f}\\n\")\n",
    "\n",
    "learning_rate = ___________  # Fill in: initial learning rate lower than 0.1\n",
    "\n",
    "# Run the gradient descent function\n",
    "min_coords, min_value, iterations = newton_raphson(function, gradient, hessian, initial_x, initial_y, learning_rate)\n",
    "\n",
    "print(f\"Minimum found at: x = {min_coords[0]:.4f}, y = {min_coords[1]:.4f}\")\n",
    "print(f\"Function value at minimum: {min_value:.4f}\")\n",
    "print(f\"Number of iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** **<span style=\"color:red\">How does the choice of initial coordinates affect the likelihood of finding the global minimum versus getting stuck in a local minimum? Why might the Newton-Raphson method converge quickly in some cases and slowly or not at all in others? Discuss how the function's shape (e.g., local minima, steepness, flat regions) and the choice of starting point influence the optimization outcome.</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** **<span style=\"color:red\">The Newton-Raphson method relies heavily on the Hessian matrix to determine the step size and direction. What could happen if the Hessian is nearly singular or not positive definite at certain points? How would this affect convergence, and what might you do to address such situations in practice?</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** **<span style=\"color:red\">The Newton-Raphson method uses both first and second derivatives to find the minimum. How might the method behave differently if the function has sharp curves, flat regions, or discontinuities? Why might this sensitivity make the method more or less effective for different types of functions?</span>**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise: Implementing a Hybrid Optimization Method\n",
    "\n",
    "To further explore optimization techniques, try implementing a **hybrid optimization method** that combines Gradient Descent and the Newton-Raphson method. The idea is to start with Gradient Descent for a few iterations to get close to the minimum and then switch to the Newton-Raphson method to refine the result more quickly. This hybrid approach can help overcome some limitations of each method individually. You might also want to think about adding a learning rate to the Newton-Raphson algorithm to help convergence. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
